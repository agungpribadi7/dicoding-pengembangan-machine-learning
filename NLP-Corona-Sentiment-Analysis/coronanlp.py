# -*- coding: utf-8 -*-
"""CoronaNLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1siENOHigP8ZhNcYKAIwRK1B6DfjIkc32
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive/')
!pip install tweet-preprocessor

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
!pip install tensorflow-addons
import tensorflow_addons as tfa
import re
import preprocessor as p
import string
import tensorflow as tf
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
import matplotlib.pyplot as plt
!pip install -q kaggle
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d datatattle/covid-19-nlp-text-classification
!mkdir corona
!unzip covid-19-nlp-text-classification.zip -d corona
!ls corona

train_data = pd.read_csv('corona/Corona_NLP_train.csv', encoding='L1')
train_data.head()

ps = PorterStemmer()
train_data.head()

x = []
y = []
jumlahData = 0
list_kata = []

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')

STOPWORDS = set(stopwords.words('english'))

nExPositive = 0
nPositive = 0
nExNegative = 0
nNegative = 0
nNeutral = 0
pattern=r'[$+0-9]'
for _, data in train_data.iterrows():
  text = p.clean(data[4])
  text = text.split(' ')
  textstr = ''
  for j in range(len(text)):
    textstr += ' '+ps.stem(text[j])
  text = textstr.strip()
  text = re.sub(pattern,'', text)
  text = REPLACE_BY_SPACE_RE.sub(' ', text)
  text = BAD_SYMBOLS_RE.sub('', text)
  text = ' '.join(word for word in text.split() if word not in STOPWORDS)

  x.append(text)
  labelku = ''

  if(data[5] == 'Positive'):
    nPositive += 1
  elif(data[5] == 'Negative'):
    nNegative += 1

  if(data[5] == 'Extremely Positive'):
    nExPositive += 1
    labelku = 'Positive'
  elif(data[5] == 'Extremely Negative'):
    nExNegative += 1
    labelku = 'Negative'
  elif(data[5] == 'Neutral'):
    nNeutral += 1
    labelku = data[5]
  else:
    labelku = data[5]
  y.append(labelku)
  


fig = plt.figure(figsize=(16, 4))
plt.bar(x=['Extremely Positive', 'Positive', 'Neutral', 'Negative', 'Extremely Negative'],
        height=[nExPositive, nPositive, nNeutral, nNegative, nExNegative],
        width=0.5)
plt.title('Data Distribution')
plt.show()

dataPosCampuran = nExPositive + nPositive
dataNegCampuran = nExNegative + nNegative
print(dataPosCampuran, dataNegCampuran, nNeutral)

ctrPos = 0
ctrNeu = 0
ctrNeg = 0

train = []
label = []
for i in range(len(x)):
  add = 0
  if(ctrPos < nNeutral and y[i] == 'Positive'):
    add = 1
    ctrPos += 1
  if(ctrNeg < nNeutral and y[i] == 'Negative'):
    add = 1
    ctrNeg += 1
  if(ctrNeu < nNeutral and y[i] == 'Neutral'):
    add = 1
    ctrNeu += 1
  if(add == 1):
    train.append(x[i])
    label.append(y[i])

train = np.array(train)
label = np.array(label)
print(train.shape)
print(label.shape)

fig = plt.figure(figsize=(16, 4))
plt.bar(x=['Positive', 'Neutral', 'Negative'],
        height=[ctrPos, ctrNeu, ctrNeg],
        width=0.5)
plt.title('Data Distribution')
plt.show()

for i in range(len(train)):
  list_kata.append(train[i].split(' '))

print(train)
print(label)

max_len_pad = 0
for i in range(len(list_kata)):
  if(max_len_pad < len(list_kata[i])):
    max_len_pad = len(list_kata[i])

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train) 

vocab_size = len(tokenizer.word_index) + 1
sekuens = tokenizer.texts_to_sequences(train)
padded = pad_sequences(sekuens, maxlen=max_len_pad, padding='post') 


from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
label = labelencoder.fit_transform(label)
label = tf.keras.utils.to_categorical(label)

kalimat_latih, kalimat_test, y_latih, y_test = train_test_split(padded, label, test_size=0.2)
print(kalimat_latih)
print(y_latih)

print(kalimat_latih.shape)

model_conv = Sequential()
model_conv.add(Embedding(vocab_size, 64, input_length=max_len_pad))
model_conv.add(tf.keras.layers.Dropout(0.5))
model_conv.add(tf.keras.layers.Conv1D(128, 5, activation='relu'))
model_conv.add(tf.keras.layers.BatchNormalization(axis=1))
model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))
model_conv.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout = 0.5, recurrent_dropout = 0.5)))
model_conv.add(tf.keras.layers.Dense(64))
model_conv.add(tf.keras.layers.Dropout(0.5))
model_conv.add(tf.keras.layers.Dense(32))
model_conv.add(tf.keras.layers.Dropout(0.5))
model_conv.add(tf.keras.layers.Dense(16))
model_conv.add(tf.keras.layers.Dropout(0.5))
model_conv.add(tf.keras.layers.Dense(3, activation='softmax'))
model_conv.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
print(model_conv.summary())

history = model_conv.fit(kalimat_latih, y_latih, validation_data=(kalimat_test, y_test), epochs=30, batch_size=128, verbose=1)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()