# -*- coding: utf-8 -*-
"""TimeSeriesSubmissionFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MzY8kS5x-lJtkH-WboLTLzlzCNetHNHr
"""

from google.colab import drive
drive.mount('/content/drive/')
import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from keras.callbacks import ReduceLROnPlateau

!nvidia-smi

!mkdir -p ~/.kaggle
!mkdir dataset
#upload kaggle.json manually
!cp ./dataset/kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d szrlee/stock-time-series-20050101-to-20171231

!unzip stock-time-series-20050101-to-20171231.zip -d dataset
!ls dataset

train_data = pd.read_csv('dataset/AAPL_2006-01-01_to_2018-01-01.csv', encoding='L1')
train_data.head()

train_data.isnull().sum()

dates = train_data['Date'].values
tempOpen  = train_data['Open'].values
tempHigh = train_data['High'].values
tempLow = train_data['Low'].values
tempClose = train_data['Close'].values

minMae = ((tempOpen.max() - tempOpen.min()) * 10/100) + tempOpen.min()
 
import plotly.graph_objects as go
!pip install chart_studio
from plotly.offline import init_notebook_mode, iplot
 
data_plot = pd.read_csv('dataset/AAPL_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'])
data_plot.head()
trace = go.Ohlc(x=data_plot['2008'].index,
                open=data_plot['2008'].Open,
                high=data_plot['2008'].High,
                low=data_plot['2008'].Low,
                close=data_plot['2008'].Close)
data = [trace]
print('Min MAE : '+str(minMae))
print('Min MAE : '+str(minMae))
print('Min MAE : '+str(minMae))
iplot(data, filename='simple_ohlc')

import cv2

print('Sebelum normalization')
print(tempOpen[:10])
out = np.zeros(tempOpen.shape, np.double)
tempOpen = cv2.normalize(tempOpen, out, 1.0, 0.0, cv2.NORM_MINMAX)
print('Setelah normalization')
print(tempOpen[:10])

size_arr = tempOpen.shape[0]
delapanPuluhPersen = int(80 * size_arr / 100)

val_open = tempOpen[delapanPuluhPersen:]
train_open = tempOpen[:delapanPuluhPersen]


#val_close = tempClose[delapanPuluhPersen:]
#train_close = tempClose[:delapanPuluhPersen]

def windowed_dataset(trainOpen, valOpen, window_size, batch_size, shuffle_buffer):
    seriesOpen = trainOpen
    seriesOpen = tf.expand_dims(seriesOpen, axis=-1)
    dsOpen = tf.data.Dataset.from_tensor_slices(seriesOpen)
    dsOpen = dsOpen.window(window_size + 1, shift=1, drop_remainder=True)
    dsOpen = dsOpen.flat_map(lambda w: w.batch(window_size + 1))
    dsOpen = dsOpen.shuffle(shuffle_buffer)
    dsOpen = dsOpen.map(lambda w: (w[:-1], w[-1:]))
    dsOpen = dsOpen.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)

    seriesOpen2 = tf.expand_dims(valOpen, axis=-1)
    valOpen = tf.data.Dataset.from_tensor_slices(seriesOpen2)
    valOpen = valOpen.window(window_size + 1, shift=1, drop_remainder=True)
    valOpen = valOpen.flat_map(lambda w: w.batch(window_size + 1))
    valOpen = valOpen.shuffle(shuffle_buffer)
    valOpen = valOpen.map(lambda w: (w[:-1], w[-1:]))
    valOpen = valOpen.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)
    
    #data dibagi menjadi banyak 100 jendela, 
    #1 jendela berisi 60 data, urutan jendela dishuffle, y adalah nilai terakhir dari tiap jendela, makanya window_size +1, +1 nya untuk y
    return [dsOpen, valOpen]

dataOpen, valOpen = windowed_dataset(train_open, val_open, window_size=60, batch_size=100, shuffle_buffer=1000)

print("Size train Open : "+str(len(train_open)))
print("Size validation Open : "+str(len(val_open)))

from tensorflow.keras import Model, Sequential

model_conv = Sequential()
model_conv.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout = 0.4, recurrent_dropout = 0.4, return_sequences=True)))
model_conv.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout = 0.4, recurrent_dropout = 0.4, return_sequences=True)))
model_conv.add(tf.keras.layers.Conv1D(128, 3, padding='same', activation='relu'))
model_conv.add(tf.keras.layers.BatchNormalization(axis=-1))
model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=2))
model_conv.add(tf.keras.layers.Dense(64, activation='relu'))
model_conv.add(tf.keras.layers.Dropout(0.4))
model_conv.add(tf.keras.layers.Dense(32, activation='relu'))
model_conv.add(tf.keras.layers.Dropout(0.4))
model_conv.add(tf.keras.layers.Dense(16, activation='relu'))
model_conv.add(tf.keras.layers.Dropout(0.4))
model_conv.add(tf.keras.layers.Dense(1))

optimizer = tf.keras.optimizers.Adam(lr=0.0001)
model_conv.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

print('Min MAE : '+str(minMae))

from tensorflow.keras.callbacks import ModelCheckpoint
filepath="/content/gdrive/My Drive/TimeSeries4Bidirec2Conv04Dropoutfinal.h5"
checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True, mode='min')

history = model_conv.fit(dataOpen,epochs=200, validation_data=valOpen, callbacks = checkpoint)

"""# **Best val_mae : 0.09956**"""

print(model_conv.summary())

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

from tensorflow.keras import Model, Sequential

model_conv = Sequential()
model_conv.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout = 0.5, recurrent_dropout = 0.5, return_sequences=True)))
model_conv.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout = 0.5, recurrent_dropout = 0.5, return_sequences=True)))
model_conv.add(tf.keras.layers.Conv1D(128, 3, padding='same', activation='relu'))
model_conv.add(tf.keras.layers.BatchNormalization(axis=-1))
model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=2))
model_conv.add(tf.keras.layers.Dense(64, activation='relu'))
model_conv.add(tf.keras.layers.Dropout(0.5))
model_conv.add(tf.keras.layers.Dense(32, activation='relu'))
model_conv.add(tf.keras.layers.Dropout(0.5))
model_conv.add(tf.keras.layers.Dense(16, activation='relu'))
model_conv.add(tf.keras.layers.Dropout(0.5))
model_conv.add(tf.keras.layers.Dense(1))

optimizer = tf.keras.optimizers.Adam(lr=0.0001)
model_conv.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])